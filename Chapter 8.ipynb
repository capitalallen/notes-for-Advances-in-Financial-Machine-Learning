{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# get project dir\n",
    "# import standard libs\n",
    "from IPython.display import display\n",
    "from IPython.core.debugger import set_trace as bp\n",
    "from pathlib import PurePath, Path\n",
    "import sys\n",
    "import time\n",
    "from collections import OrderedDict as od\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "\n",
    "pp = PurePath(Path.cwd()).parts[:]\n",
    "pdir = PurePath(*pp)\n",
    "data_script_dir = pdir / 'src' / 'data'\n",
    "bars_script_dir = pdir / 'src' / 'features'\n",
    "sys.path.append(data_script_dir.as_posix())\n",
    "sys.path.append(bars_script_dir.as_posix())\n",
    "viz_dir = pdir / 'reports' / 'figures'\n",
    "data_dir = pdir / 'data'\n",
    "\n",
    "# import python scientific stack\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 100)\n",
    "from dask import dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "from multiprocessing import cpu_count\n",
    "pbar = ProgressBar()\n",
    "pbar.register()\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from numba import jit\n",
    "import math\n",
    "\n",
    "# import visual tools\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-talk')\n",
    "plt.style.use('bmh')\n",
    "#plt.rcParams['font.family'] = 'DejaVu Sans Mono'\n",
    "plt.rcParams['font.size'] = 9.5\n",
    "plt.rcParams['font.weight'] = 'medium'\n",
    "plt.rcParams['figure.figsize'] = 10,7\n",
    "blue, green, red, purple, gold, teal = sns.color_palette('colorblind', 6)\n",
    "\n",
    "# import util libs\n",
    "# from tqdm import tqdm, tqdm_notebook\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from utils import *\n",
    "from bars import *\n",
    "from labelling import *\n",
    "from mpEngine import *\n",
    "from sampleWeights import *\n",
    "from ffd import *\n",
    "from cvFin import *\n",
    "from featureImportance import *\n",
    "RANDOM_STATE = 777\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.2953597377909"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infp=PurePath(data_dir/'processed'/'clean_IVE_fut_prices.parq')\n",
    "df = pd.read_parquet(infp)\n",
    "df = df.assign(dates = df.index)\n",
    "dbars = dollar_bar_df(df, 'dv', 1_000_000)\n",
    "dbars = dbars.drop(['dates'], axis = 1)\n",
    "# x = np.log(dbars.price).cumsum()\n",
    "x = dbars.price.cumsum()\n",
    "x = x[~x.index.duplicated()]\n",
    "dfx2 = fracDiff_FFD(x.to_frame(),2)\n",
    "\n",
    "joined = dfx2.join(x.rename('original'),how='left')\n",
    "joined.corr()\n",
    "coint_pval = sm.tsa.stattools.coint(joined.price, joined.original)[1]\n",
    "dfx2 = -dfx2[~dfx2.index.duplicated()]\n",
    "\n",
    "ffd_std = dfx2.std()[0]\n",
    "\n",
    "ffd_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "dataframe information\n",
      "-------------------------------------------------------------------------------\n",
      "                        dbars  frac_diff_feat\n",
      "dates                                        \n",
      "2018-10-02 11:49:10  116.4980        116.1370\n",
      "2018-10-02 12:12:54  116.5699         -0.0719\n",
      "2018-10-04 13:44:24  116.1600       -116.1610\n",
      "2018-10-04 13:54:45  116.0337        116.1663\n",
      "2018-10-04 14:01:44  116.0100          0.0237\n",
      "--------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 1677 entries, 2009-11-03 11:20:59 to 2018-10-04 14:01:44\n",
      "Data columns (total 2 columns):\n",
      "dbars             1677 non-null float64\n",
      "frac_diff_feat    1677 non-null float64\n",
      "dtypes: float64(2)\n",
      "memory usage: 39.3 KB\n",
      "None\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-28 11:25:17.302596 100.0% applyPtSlOnT1 done after 0.01 minutes. Remaining 0.0 minutes..\n",
      "2018-10-28 11:25:17.583000 100.0% mpNumCoEvents done after 0.0 minutes. Remaining 0.0 minutes..\n",
      "2018-10-28 11:25:17.847053 100.0% mpSampleTW done after 0.0 minutes. Remaining 0.0 minutes..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped label:  0.0 0.0012338062924120913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-28 11:25:18.236623 14.29% mpSampleW done after 0.0 minutes. Remaining 0.01 minutes.\r",
      "2018-10-28 11:25:18.275094 28.57% mpSampleW done after 0.0 minutes. Remaining 0.01 minutes.\r",
      "2018-10-28 11:25:18.276405 42.86% mpSampleW done after 0.0 minutes. Remaining 0.0 minutes.\r",
      "2018-10-28 11:25:18.279181 57.14% mpSampleW done after 0.0 minutes. Remaining 0.0 minutes.\r",
      "2018-10-28 11:25:18.279471 71.43% mpSampleW done after 0.0 minutes. Remaining 0.0 minutes.\r",
      "2018-10-28 11:25:18.280019 85.71% mpSampleW done after 0.0 minutes. Remaining 0.0 minutes.\r",
      "2018-10-28 11:25:18.280050 100.0% mpSampleW done after 0.0 minutes. Remaining 0.0 minutes.\n"
     ]
    }
   ],
   "source": [
    "def getTEvents2(gRaw, h, symmetric = True, isReturn = False):\n",
    "    \"\"\"\n",
    "    Symmetric CUSUM Filter\n",
    "    Sample a bar t iff S_t >= h at which point S_t is reset\n",
    "    Multiple events are not triggered by gRaw hovering around a threshold level\n",
    "    It will require a full run of length h for gRaw to trigger an event\n",
    "    \n",
    "    Two arguments:\n",
    "        gRaw: the raw time series we wish to filter (gRaw), e.g. return\n",
    "        h: threshold\n",
    "        \n",
    "    Return:\n",
    "        pd.DatatimeIndex.append(tEvents): \n",
    "    \"\"\"\n",
    "    tEvents, sPos, sNeg = [], 0, 0\n",
    "    if isReturn:\n",
    "        diff = gRaw\n",
    "    else:\n",
    "        diff = gRaw.diff()\n",
    "    if symmetric:\n",
    "        if np.shape(h) == ():\n",
    "\n",
    "            for i in diff.index[1:]:\n",
    "                tmp = diff.loc[i].mean()\n",
    "                sPos, sNeg = max(0,sPos+tmp), min(0,sNeg+tmp)\n",
    "                if sNeg < -h and tradableHour(i):\n",
    "                    sNeg = 0; tEvents.append(i)\n",
    "                elif sPos > h and tradableHour(i):\n",
    "                    sPos = 0; tEvents.append(i)\n",
    "        else:\n",
    "            for i in diff.index[1:]:\n",
    "                tmp = diff.loc[i].mean()\n",
    "                sPos, sNeg = max(0,sPos+tmp), min(0,sNeg+tmp)\n",
    "                if sNeg < -h[i] and tradableHour(i):\n",
    "                    sNeg = 0; tEvents.append(i)\n",
    "                elif sPos > h[i] and tradableHour(i):\n",
    "                    sPos = 0; tEvents.append(i)\n",
    "    else:\n",
    "        if np.shape(h) == ():\n",
    "\n",
    "            for i in diff.index[1:]:\n",
    "                tmp = diff.loc[i].mean()\n",
    "                sAbs = sAbs+tmp\n",
    "                if sAbs > h and tradableHour(i):\n",
    "                    sNeg = 0; tEvents.append(i)\n",
    "                \n",
    "        else:\n",
    "            for i in diff.index[1:]:\n",
    "                tmp = diff.loc[i].mean()\n",
    "                sAbs = sAbs+tmp\n",
    "                if sAbs > h[i] and tradableHour(i):\n",
    "                    sNeg = 0; tEvents.append(i)\n",
    "            \n",
    "    return pd.DatetimeIndex(tEvents)\n",
    "\n",
    "def getDailyVol2(close,span0=100):\n",
    "    # daily vol reindexed to close\n",
    "    df0=close.index.searchsorted(close.index-pd.Timedelta(days=1))\n",
    "    #bp()\n",
    "    df0=df0[df0>0]\n",
    "    #bp()\n",
    "    df0=(pd.Series(close.index[df0-1],\n",
    "                   index=close.index[close.shape[0]-df0.shape[0]:]))\n",
    "    #bp()\n",
    "    try:\n",
    "        df0=close.loc[df0.index]/close.loc[df0.values].values-1 # daily rets\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('adjusting shape of close.loc[df0.index]')\n",
    "        cut = close.loc[df0.index].shape[0] - close.loc[df0.values].shape[0]\n",
    "        # I dont't think it make sense\n",
    "        df0=close.loc[df0.index].iloc[:-cut]/close.loc[df0.values].values-1\n",
    "    df0=df0.ewm(span=span0).std().rename('dailyVol')\n",
    "    return df0\n",
    "\n",
    "def evaluate(X,y,clf):\n",
    "    from sklearn import metrics\n",
    "    # The random forest model by itself\n",
    "    y_pred_rf = clf.predict_proba(X)[:, 1]\n",
    "    y_pred = clf.predict(X)\n",
    "    fpr_rf, tpr_rf, _ = metrics.roc_curve(y, y_pred_rf)\n",
    "    print(metrics.classification_report(y, y_pred))\n",
    "\n",
    "    plt.figure(figsize=(9,6))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr_rf, tpr_rf, label='clf')\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "tEvents = getTEvents2(dfx2,h=ffd_std)\n",
    "dbars_feat = dbars.price.loc[tEvents]\n",
    "frac_diff_feat = dfx2.loc[tEvents]\n",
    "ftMtx = (pd.DataFrame()\n",
    "         .assign(dbars=dbars_feat,\n",
    "                 frac_diff_feat=frac_diff_feat)\n",
    "         .drop_duplicates().dropna())\n",
    "cprint(ftMtx)\n",
    "\n",
    "ftMtx = ftMtx[~ftMtx.index.duplicated()]\n",
    "dailyVol = getDailyVol2(ftMtx.dbars)\n",
    "t1 = addVerticalBarrier(tEvents, ftMtx.dbars, hour=120)\n",
    "\n",
    "ptsl = [1,1]\n",
    "#ptsl = [daily]\n",
    "target=dailyVol*2\n",
    "# select minRet\n",
    "minRet = 0.01\n",
    "# get cpu count - 1\n",
    "cpus = cpu_count() - 1\n",
    "events = getEvents(ftMtx.dbars,tEvents,ptsl,target,minRet,cpus,t1=t1)\n",
    "\n",
    "close=ftMtx.dbars\n",
    "out = get_Concur_Uniqueness(close,events,cpus)\n",
    "# get avg uniqueness for bootstrapping\n",
    "avgU = out['tW'].mean()\n",
    "labels = getBins(events, ftMtx.dbars)\n",
    "clean_labels = dropLabels(labels)\n",
    "trgt = clean_labels.bin\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, BaggingClassifier\n",
    "\n",
    "# model data# model  \n",
    "data = ftMtx.join(out,how='left').join(trgt,how='left').dropna()\n",
    "data_ = data.drop(['t1'],axis = 1)\n",
    "X = data_.iloc[:,:-1].values\n",
    "y = data_.iloc[:,-1].values.reshape(-1,1)\n",
    "XX = data_.iloc[:,:-1]\n",
    "yy = data_.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(trnsX, cont, n_estimators = 1000, cv = 10):\n",
    "    # arguments\n",
    "    dict0 = {'minWLeaf': [0.], 'scoring': ['accuracy'], 'method': ['MDI','MDA', 'SFI'], 'max_samples': [1.]}    \n",
    "    # split dict0 into 3 different jobs (by method)\n",
    "    jobs =(dict(zip(dict0, i)) for i in product(*dict0.values()))\n",
    "    out = [] # empty list\n",
    "    # key arguments\n",
    "    kargs = {'pathOut': './testFunc/', 'n_estimators': n_estimators, 'tag': 'testFunc', 'cv': cv}\n",
    "    for job in jobs: # for each jobs\n",
    "        # job params\n",
    "        job['simNum'] = job['method'] + '_' + job['scoring'] + '_'+ '%.2f'%job['minWLeaf'] + '_' + str(job['max_samples'])\n",
    "        print (job['simNum']) # print job params\n",
    "        kargs.update(job) # update/add the elemets to the dictionary\n",
    "        imp, oob, oos = featImportance(trnsX = trnsX, cont = cont, **kargs) #  find faeture importance using imp, oob, oos\n",
    "        plotFeatImportance(imp = imp, oob = oob, oos = oos, **kargs) # plot the feature importance\n",
    "        df0 = imp[['mean']] / imp['mean'].abs().sum() # normalised\n",
    "        df0['type'] = [i[0] for i in df0.index] # \n",
    "        df0 = df0.groupby('type')['mean'].sum().to_dict() \n",
    "        df0.update({'oob': oob, 'oos': oos}) # update/add the elemets to the dictionary\n",
    "        df0.update(job) # update/add the elemets to the dictionary\n",
    "        out.append(df0) # append df0 to out\n",
    "    out = pd.DataFrame(out).sort_values(['method', 'scoring', 'minWLeaf', 'max_samples']) # sort the df by\n",
    "#     # only the followings are output\n",
    "    out = out[['method', 'scoring', 'minWLeaf', 'max_samples', 'I', 'R', 'N', 'oob', 'oos']]\n",
    "#    # out = out['method', 'scoring', 'minWLeaf', 'max_samples', 'oob', 'oos']\n",
    "    out.to_csv(kargs['pathOut'] + 'stats.csv')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-28 11:25:25.183056 75.0% auxFeatImpSFI done after 0.06 minutes. Remaining 0.02 minutes.\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    mean std\n",
      "tW              0.543083   0\n",
      "w               0.529795   0\n",
      "dbars           0.477375   0\n",
      "frac_diff_feat  0.451235   0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-28 11:25:26.389552 100.0% auxFeatImpSFI done after 0.08 minutes. Remaining 0.0 minutes.\n"
     ]
    }
   ],
   "source": [
    "SFI_imp, SFI_oob, SFI_oos = featImportance(XX, data[['bin','w','t1']], method = 'SFI')\n",
    "\n",
    "print(SFI_imp.sort_values(by=['mean'], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    mean       std\n",
      "w               0.312872  0.001526\n",
      "dbars           0.286996  0.001430\n",
      "frac_diff_feat  0.213445  0.001166\n",
      "tW              0.186688  0.000874\n"
     ]
    }
   ],
   "source": [
    "MDI_imp, MDI_oob, MDI_oos = featImportance(XX, data[['bin','w','t1']], method = 'MDI')\n",
    "\n",
    "print(MDI_imp.sort_values(by=['mean'], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    mean       std\n",
      "w               0.317509  0.001478\n",
      "dbars           0.284453  0.001373\n",
      "frac_diff_feat  0.213380  0.001123\n",
      "tW              0.184658  0.000877\n"
     ]
    }
   ],
   "source": [
    "MDA_imp, MDA_oob, MDA_oos = featImportance(XX, data[['bin','w','t1']], method = 'MDI')\n",
    "print(MDA_imp.sort_values(by=['mean'], ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaX = orthoFeats(XX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaX = pd.DataFrame(pcaX,index = XX.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaX.columns = [(\"pc\" + str(i+1)) for i in pcaX.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-28 11:36:44.965493 100.0% auxFeatImpSFI done after 0.07 minutes. Remaining 0.0 minutes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         mean std\n",
      "pc4   0.61986   0\n",
      "pc2  0.531624   0\n",
      "pc3  0.496696   0\n",
      "pc1  0.490896   0\n"
     ]
    }
   ],
   "source": [
    "SFI_imp, SFI_oob, SFI_oos = featImportance(pcaX, data[['bin','w','t1']], method = 'SFI')\n",
    "\n",
    "print(SFI_imp.sort_values(by=['mean'], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         mean       std\n",
      "pc1  0.275558  0.001453\n",
      "pc4  0.266737  0.001547\n",
      "pc3  0.243212  0.001119\n",
      "pc2  0.214493  0.001123\n"
     ]
    }
   ],
   "source": [
    "MDI_imp, MDI_oob, MDI_oos = featImportance(pcaX, data[['bin','w','t1']], method = 'MDI')\n",
    "\n",
    "print(MDI_imp.sort_values(by=['mean'], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         mean       std\n",
      "pc1  0.278535  0.001459\n",
      "pc4  0.262368  0.001479\n",
      "pc3  0.243087  0.001138\n",
      "pc2  0.216011  0.001150\n"
     ]
    }
   ],
   "source": [
    "MDA_imp, MDA_oob, MDA_oos = featImportance(pcaX, data[['bin','w','t1']], method = 'MDI')\n",
    "print(MDA_imp.sort_values(by=['mean'], ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pcaX merge XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinX = XX.join(pcaX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-28 11:42:08.149976 87.5% auxFeatImpSFI done after 0.1 minutes. Remaining 0.01 minutes..\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    mean std\n",
      "pc4              0.61986   0\n",
      "tW              0.543083   0\n",
      "pc2             0.531624   0\n",
      "w               0.529795   0\n",
      "pc3             0.496696   0\n",
      "pc1             0.490896   0\n",
      "dbars           0.477375   0\n",
      "frac_diff_feat  0.451235   0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-28 11:42:08.383152 100.0% auxFeatImpSFI done after 0.11 minutes. Remaining 0.0 minutes.\n"
     ]
    }
   ],
   "source": [
    "SFI_imp, SFI_oob, SFI_oos = featImportance(joinX, data[['bin','w','t1']], method = 'SFI')\n",
    "\n",
    "print(SFI_imp.sort_values(by=['mean'], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    mean       std\n",
      "w               0.149303  0.001427\n",
      "dbars           0.142741  0.001202\n",
      "pc1             0.139010  0.001298\n",
      "pc4             0.133645  0.001210\n",
      "pc3             0.125986  0.001004\n",
      "pc2             0.107286  0.000833\n",
      "frac_diff_feat  0.104613  0.000867\n",
      "tW              0.097417  0.000714\n"
     ]
    }
   ],
   "source": [
    "MDI_imp, MDI_oob, MDI_oos = featImportance(joinX, data[['bin','w','t1']], method = 'MDI')\n",
    "\n",
    "print(MDI_imp.sort_values(by=['mean'], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    mean       std\n",
      "w               0.148162  0.001384\n",
      "dbars           0.142745  0.001218\n",
      "pc1             0.138361  0.001314\n",
      "pc4             0.136027  0.001261\n",
      "pc3             0.127085  0.000967\n",
      "pc2             0.106574  0.000826\n",
      "frac_diff_feat  0.105005  0.000833\n",
      "tW              0.096041  0.000726\n"
     ]
    }
   ],
   "source": [
    "MDA_imp, MDA_oob, MDA_oos = featImportance(joinX, data[['bin','w','t1']], method = 'MDI')\n",
    "print(MDA_imp.sort_values(by=['mean'], ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerun joinX after removing the most important features the previous section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_SFI = joinX[SFI_imp.sort_values(by=['mean'], ascending=False)[1:].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-28 11:49:36.230492 100.0% auxFeatImpSFI done after 0.12 minutes. Remaining 0.0 minutes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    mean std\n",
      "pc4              0.61986   0\n",
      "tW              0.543083   0\n",
      "pc2             0.531624   0\n",
      "w               0.529795   0\n",
      "pc3             0.496696   0\n",
      "pc1             0.490896   0\n",
      "dbars           0.477375   0\n",
      "frac_diff_feat  0.451235   0\n"
     ]
    }
   ],
   "source": [
    "SFI_imp, SFI_oob, SFI_oos = featImportance(joinX, data[['bin','w','t1']], method = 'SFI')\n",
    "\n",
    "print(SFI_imp.sort_values(by=['mean'], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    mean       std\n",
      "dbars           0.163858  0.001263\n",
      "pc1             0.161836  0.001378\n",
      "pc4             0.161214  0.001351\n",
      "pc3             0.146205  0.001010\n",
      "pc2             0.128153  0.000919\n",
      "frac_diff_feat  0.126614  0.000930\n",
      "tW              0.112120  0.000766\n"
     ]
    }
   ],
   "source": [
    "joinX_MDI = joinX[MDI_imp.sort_values(by=['mean'], ascending=False)[1:].index]\n",
    "MDI_imp, MDI_oob, MDI_oos = featImportance(joinX_MDI, data[['bin','w','t1']], method = 'MDI')\n",
    "\n",
    "print(MDI_imp.sort_values(by=['mean'], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    mean       std\n",
      "pc4             0.163570  0.001295\n",
      "pc1             0.163480  0.001381\n",
      "dbars           0.161630  0.001204\n",
      "pc3             0.146044  0.001045\n",
      "pc2             0.126822  0.000916\n",
      "frac_diff_feat  0.125326  0.000926\n",
      "tW              0.113128  0.000772\n"
     ]
    }
   ],
   "source": [
    "joinX_MDA = joinX[MDA_imp.sort_values(by=['mean'], ascending=False)[1:].index]\n",
    "MDA_imp, MDA_oob, MDA_oos = featImportance(joinX_MDA, data[['bin','w','t1']], method = 'MDI')\n",
    "print(MDA_imp.sort_values(by=['mean'], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
